{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "        args = (\n",
    "        agent_num: int,\n",
    "        hid_size: int,\n",
    "        obs_size: [int],\n",
    "        continuous: bool,\n",
    "        action_dim: [int],\n",
    "        comm_iters: int,\n",
    "        init_std: float,\n",
    "        'lrate': float,\n",
    "        'batch_size': int,\n",
    "        'max_steps': int,\n",
    "        'gamma': float,\n",
    "        'mean_ratio': float,\n",
    "        'normalize_rewards': bool,\n",
    "        'advantages_per_action': bool,\n",
    "        'value_coeff': float,\n",
    "        'entr': float\n",
    "        )\n",
    "        args is a namedtuple, e.g. args = collections.namedtuple()\n",
    "        '''\n",
    "        super(CommNet, self).__init__()\n",
    "        self.args = args\n",
    "        # create a model\n",
    "        self.construct_model()\n",
    "        # initialize parameters with normal distribution with mean of 0\n",
    "        map(self.init_weights, self.parameters())\n",
    "    \n",
    "    def mask_obs(self, x):\n",
    "        x_lens = [len(x_) for x_ in x]\n",
    "        x_len_max = np.max(x_lens)\n",
    "        for i in range(len(x_lens)):\n",
    "            if x_lens[i] < x_len_max:\n",
    "                x[i] = np.concatenate((x[i], np.zeros(x_len_max-x_len)), axis=0)\n",
    "        return x\n",
    "    \n",
    "    def construct_model(self):\n",
    "        '''\n",
    "        define the model of vanilla CommNet\n",
    "        '''\n",
    "        # encoder transforms observation to latent variables\n",
    "        self.encoder = nn.Linear(self.args.obs_size, self.args.hid_size)\n",
    "        # communication mask where the diagnal should be 0\n",
    "        self.comm_mask = torch.ones(self.args.agent_num, self.args.agent_num) - torch.eye(self.args.agent_num, self.args.agent_num)\n",
    "        # decoder transforms hidden states to action vector\n",
    "        if self.args.continuous:\n",
    "            self.action_mean = nn.Linear(self.args.hid_size, self.args.action_dim)\n",
    "            self.action_log_std = nn.Parameter(torch.zeros(1, self.args.action_dim))\n",
    "        else:\n",
    "            self.action_head = nn.Linear(self.args.hid_size, self.args.action_dim)\n",
    "        # define communication inference\n",
    "        self.f_module = nn.Linear(self.args.hid_size, self.args.hid_size)\n",
    "        self.f_modules = nn.ModuleList([self.f_module for _ in range(self.args.comm_iters)])\n",
    "        # define communication encoder\n",
    "        self.C_module = nn.Linear(self.args.hid_size, self.args.hid_size)\n",
    "        self.C_modules = nn.ModuleList([self.C_module for _ in range(self.args.comm_iters)])\n",
    "        # define value function\n",
    "        self.value_head = nn.Linear(self.args.hid_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def state_encoder(self, x):\n",
    "        '''\n",
    "        define a single forward pass of communication inference\n",
    "        '''\n",
    "        return self.tanh(self.encoder(self.mask_obs(x)))\n",
    "\n",
    "    def get_agent_mask(self, batch_size, info):\n",
    "        '''\n",
    "        define the getter of agent mask to confirm the living agent\n",
    "        '''\n",
    "        n = self.args.agent_num\n",
    "        with torch.no_grad():\n",
    "            if 'alive_mask' in info:\n",
    "                agent_mask = torch.from_numpy(info['alive_mask'])\n",
    "                num_agents_alive = agent_mask.sum()\n",
    "            else:\n",
    "                agent_mask = torch.ones(n)\n",
    "                num_agents_alive = n\n",
    "        # shape = (1, 1, n)\n",
    "        agent_mask = agent_mask.view(1, 1, n)\n",
    "        # shape = (batch_size, n ,n, 1)\n",
    "        agent_mask = agent_mask.expand(batch_size, n, n).unsqueeze(-1)\n",
    "        return num_agents_alive, agent_mask\n",
    "\n",
    "    def action(self, obs, info={}):\n",
    "        '''\n",
    "        define the action process of vanilla CommNet\n",
    "        '''\n",
    "        length = 0\n",
    "        for o in obs:\n",
    "            if o.shape[0] > length:\n",
    "                length = o.shape[0]\n",
    "        i = 0\n",
    "        for o in obs:\n",
    "            if o.shape[0] < length:\n",
    "                obs[i] = np.concatenate((o, np.zeros(length-o.shape[0])))\n",
    "            i += 1\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(np.array(obs)).float().unsqueeze(0)\n",
    "        # encode observation\n",
    "        h = self.state_encoder(obs)\n",
    "        # get the batch size\n",
    "        batch_size = obs.size()[0]\n",
    "        # get the total number of agents including dead\n",
    "        n = self.args.agent_num\n",
    "        # get the agent mask\n",
    "        num_agents_alive, agent_mask = self.get_agent_mask(batch_size, info)\n",
    "        # conduct the main process of communication\n",
    "        for i in range(self.args.comm_iters):\n",
    "            # shape = (batch_size, n, hid_size)->(batch_size, n, 1, hid_size)->(batch_size, n, n, hid_size)\n",
    "            h_ = h.unsqueeze(-2).expand(-1, n, n, self.args.hid_size)\n",
    "            # construct the communication mask\n",
    "            mask = self.comm_mask.view(1, n, n) # shape = (1, n, n)\n",
    "            mask = mask.expand(batch_size, n, n) # shape = (batch_size, n, n)\n",
    "            mask = mask.unsqueeze(-1) # shape = (batch_size, n, n, 1)\n",
    "            mask = mask.expand_as(h_) # shape = (batch_size, n, n, hid_size)\n",
    "            # mask each agent itself (collect the hidden state of other agents)\n",
    "            h_ = h_ * mask\n",
    "            # mask the dead agent\n",
    "            h_ = h_ * agent_mask * agent_mask.transpose(1, 2)\n",
    "            # average the hidden state\n",
    "            h_ = h_ / (num_agents_alive - 1)\n",
    "            # calculate the communication vector\n",
    "            c = h_.sum(dim=1) if i != 0 else torch.zeros_like(h) # shape = (batch_size, n, hid_size)\n",
    "            # h_{j}^{i+1} = \\sigma(H_j * h_j^{i+1} + C_j * c_j^{i+1})\n",
    "            h = self.tanh(sum([self.f_modules[i](h), self.C_modules[i](c)]))\n",
    "        # calculate the value function (critic)\n",
    "        value_head = self.value_head(h)\n",
    "        # calculate the action vector (actor)\n",
    "        if self.args.continuous:\n",
    "            # shape = (batch_size, n, action_dim)\n",
    "            action_mean = self.action_mean(h)\n",
    "            action_log_std = self.action_log_std.expand_as(action_mean)\n",
    "            action_std = torch.exp(action_log_std)\n",
    "            # will be used later to sample\n",
    "            action = (action_mean, action_log_std, action_std)\n",
    "        else:\n",
    "            # discrete actions, shape = (batch_size, n, action_type, action_num)\n",
    "            action = F.log_softmax(self.action_head(h), dim=-1)\n",
    "        return action, value_head\n",
    "    \n",
    "    def forward(self, obs, info={}):\n",
    "        return self.action(obs, info)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        '''\n",
    "        initialize the weights of parameters\n",
    "        '''\n",
    "        if type(m) == nn.Linear:\n",
    "            m.weight.data.normal_(0, self.args.init_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from inspect import getargspec\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stat(src, dest):\n",
    "    for k, v in src.items():\n",
    "        if not k in dest:\n",
    "            dest[k] = v\n",
    "        elif isinstance(v, numbers.Number):\n",
    "            dest[k] = dest.get(k, 0) + v\n",
    "        elif isinstance(v, np.ndarray): # for rewards in case of multi-agent\n",
    "            dest[k] = dest.get(k, 0) + v\n",
    "        else:\n",
    "            if isinstance(dest[k], list) and isinstance(v, list):\n",
    "                dest[k].extend(v)\n",
    "            elif isinstance(dest[k], list):\n",
    "                dest[k].append(v)\n",
    "            else:\n",
    "                dest[k] = [dest[k], v]\n",
    "\n",
    "def normal_entropy(std):\n",
    "    var = std.pow(2)\n",
    "    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n",
    "    return entropy.sum(1, keepdim=True)\n",
    "\n",
    "def normal_log_density(x, mean, log_std, std):\n",
    "    var = std.pow(2)\n",
    "    log_density = -(x - mean).pow(2) / (2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n",
    "    return log_density.sum(1, keepdim=True)\n",
    "\n",
    "def multinomials_log_density(actions, log_probs):\n",
    "    log_prob = 0\n",
    "    for i in range(len(log_probs)):\n",
    "        log_prob += log_probs[i].gather(1, actions[:, i].long().unsqueeze(1))\n",
    "    return log_prob\n",
    "\n",
    "def multinomials_log_densities(actions, log_probs):\n",
    "    log_prob = [0] * len(log_probs)\n",
    "    for i in range(len(log_probs)):\n",
    "        log_prob[i] += log_probs[i].gather(1, actions[:, i].long().unsqueeze(1))\n",
    "    log_prob = torch.cat(log_prob, dim=-1)\n",
    "    return log_prob\n",
    "\n",
    "def select_action(args, action_out):\n",
    "    if args.continuous:\n",
    "        action_mean, _, action_std = action_out\n",
    "        action = torch.normal(action_mean, action_std)\n",
    "        return action.detach()\n",
    "    else:\n",
    "        log_p_a = action_out\n",
    "        p_a = [[z.exp() for z in x] for x in log_p_a]\n",
    "        ret = torch.stack([torch.stack([torch.multinomial(x, 1).detach() for x in p]) for p in p_a])\n",
    "        return ret\n",
    "\n",
    "def translate_action(args, env, action):\n",
    "    # This is different from the source code\n",
    "    if args.action_num > 0:\n",
    "        action_tensor = torch.zeros(tuple(action.size()[:-1])+(args.action_num,))\n",
    "        action_tensor.scatter_(-1, action, 1)\n",
    "        # environment takes discrete action\n",
    "        actual = [action_tensor[:, i, :].squeeze().data.numpy() for i in range(action_tensor.size(1))]\n",
    "        action = np.array(actual)\n",
    "        return action, actual\n",
    "    else:\n",
    "        if args.continuous:\n",
    "            action = action.data[0].numpy()\n",
    "            cp_action = action.copy()\n",
    "            # clip and scale action to correct range\n",
    "            for i in range(len(action)):\n",
    "                low = env.action_space.low[i]\n",
    "                high = env.action_space.high[i]\n",
    "                cp_action[i] = cp_action[i] * args.action_scale\n",
    "                cp_action[i] = max(-1.0, min(cp_action[i], 1.0))\n",
    "                cp_action[i] = 0.5 * (cp_action[i] + 1.0) * (high - low) + low\n",
    "            return action, cp_action\n",
    "        else:\n",
    "            actual = np.zeros(len(action))\n",
    "            for i in range(len(action)):\n",
    "                low = env.action_space.low[i]\n",
    "                high = env.action_space.high[i]\n",
    "                actual[i] = action[i].data.squeeze()[0] * (high - low) / (args.naction_heads[i] - 1) + low\n",
    "            action = [x.squeeze().data[0] for x in action]\n",
    "            return action, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transition of an episode\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'action_out', 'value', 'episode_mask', 'episode_mini_mask', 'next_state', 'reward', 'misc'))\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, args, policy_net, env):\n",
    "        self.args = args\n",
    "        self.policy_net = policy_net\n",
    "        self.env = env\n",
    "        self.optimizer = optim.RMSprop(policy_net.parameters(), lr = args.lrate, alpha=0.97, eps=1e-6)\n",
    "        self.params = [p for p in self.policy_net.parameters()]\n",
    "\n",
    "    def get_episode(self):\n",
    "        # define the episode list\n",
    "        episode = []\n",
    "        # reset the environment\n",
    "        state = self.env.reset()\n",
    "        # set up two auxilliary dictionaries\n",
    "        stat = dict()\n",
    "        info = dict()\n",
    "        # define the main process of exploration\n",
    "        for t in range(self.args.max_steps):\n",
    "            plt.imshow(np.array(self.env.render(mode='rgb_array')).squeeze())\n",
    "            display.display(plt.gcf())    \n",
    "            display.clear_output()\n",
    "            misc = dict()\n",
    "            # decide the next action and return the correlated state value (baseline)\n",
    "            action_out, value = self.policy_net.action(state, info)\n",
    "            # return the sampled actions of all of agents\n",
    "            action = select_action(self.args, action_out)\n",
    "            # return the rescaled (clipped) actions\n",
    "            _, actual = translate_action(self.args, self.env, action)\n",
    "            # receive the reward and the next state\n",
    "            next_state, reward, done, info = self.env.step(actual)\n",
    "            # record the alive agents\n",
    "            if 'alive_mask' in info:\n",
    "                # serve for the starcraft environment\n",
    "                misc['alive_mask'] = info['alive_mask'].reshape(reward.shape)\n",
    "            else:\n",
    "                misc['alive_mask'] = np.ones_like(reward)\n",
    "            # define the flag of the finish of exploration\n",
    "            done = done or t == self.args.max_steps - 1\n",
    "\n",
    "            reward = np.array(reward)\n",
    "            episode_mask = np.ones(reward.shape)\n",
    "            episode_mini_mask = np.ones(reward.shape)\n",
    "\n",
    "            if done:\n",
    "                episode_mask = np.zeros(reward.shape)\n",
    "            else:\n",
    "                # serve for traffic environment\n",
    "                if 'is_completed' in info:\n",
    "                    episode_mini_mask = 1 - info['is_completed'].reshape(-1)\n",
    "            # record a transition\n",
    "            trans = Transition(state, action, action_out, value, episode_mask, episode_mini_mask, next_state, reward, misc)\n",
    "            # record the current transition to the whole episode\n",
    "            episode.append(trans)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        stat['num_steps'] = t + 1\n",
    "        stat['steps_taken'] = stat['num_steps']\n",
    "        return (episode, stat)\n",
    "\n",
    "    def compute_grad(self, batch):\n",
    "\n",
    "        stat = dict()\n",
    "\n",
    "        action_dim = self.args.action_dim\n",
    "        n = self.args.agent_num\n",
    "        batch_size = len(batch.state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rewards = torch.Tensor(batch.reward)\n",
    "            \n",
    "            episode_masks = torch.Tensor(batch.episode_mask)\n",
    "            episode_mini_masks = torch.Tensor(batch.episode_mini_mask)\n",
    "            batch_action = torch.stack(batch.action, dim=0).float()\n",
    "            actions = torch.Tensor(batch_action)\n",
    "            actions = actions.transpose(1, 2).view(-1, n, 1)\n",
    "\n",
    "        values = torch.cat(batch.value, dim=0)\n",
    "        action_out = list(zip(*batch.action_out))\n",
    "        action_out = [torch.cat(a, dim=0) for a in action_out]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            alive_masks = torch.Tensor(np.concatenate([item['alive_mask'] for item in batch.misc])).view(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            coop_returns = torch.Tensor(batch_size, n)\n",
    "            ncoop_returns = torch.Tensor(batch_size, n)\n",
    "            returns = torch.Tensor(batch_size, n)\n",
    "            # deltas = torch.Tensor(batch_size, n)\n",
    "            advantages = torch.Tensor(batch_size, n)\n",
    "            values = values.view(batch_size, n)\n",
    "\n",
    "        prev_coop_return = 0\n",
    "        prev_ncoop_return = 0\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "        \n",
    "        # calculate the return reversely and the reward is shared\n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            coop_returns[i] = rewards[i] + self.args.gamma * prev_coop_return * episode_masks[i]\n",
    "            ncoop_returns[i] = rewards[i] + self.args.gamma * prev_ncoop_return * episode_masks[i] * episode_mini_masks[i]\n",
    "\n",
    "            prev_coop_return = coop_returns[i]\n",
    "            prev_ncoop_return = ncoop_returns[i]\n",
    "\n",
    "            returns[i] = (self.args.mean_ratio * coop_returns[i].mean()) \\\n",
    "                         + ((1 - self.args.mean_ratio) * ncoop_returns[i])\n",
    "        \n",
    "        # calculate the advantage\n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            advantages[i] = returns[i] - values.data[i]\n",
    "        \n",
    "        # normalize the advantage\n",
    "        if self.args.normalize_rewards:\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        # take the policy of the actions\n",
    "        if self.args.continuous:\n",
    "            action_means, action_log_stds, action_stds = action_out\n",
    "            log_prob = normal_log_density(actions, action_means, action_log_stds, action_stds)\n",
    "        else:\n",
    "            log_p_a = action_out\n",
    "            actions = actions.contiguous().view(-1, 1)\n",
    "            if self.args.advantages_per_action:\n",
    "                log_prob = multinomials_log_densities(actions, log_p_a)\n",
    "            else:\n",
    "                log_prob = multinomials_log_density(actions, log_p_a)\n",
    "\n",
    "        if self.args.advantages_per_action:\n",
    "            action_loss = -advantages.view(-1).unsqueeze(-1) * log_prob\n",
    "            action_loss *= alive_masks.unsqueeze(-1)\n",
    "        else:\n",
    "            action_loss = -advantages.view(-1) * log_prob.squeeze()\n",
    "            action_loss *= alive_masks\n",
    "\n",
    "        action_loss = action_loss.sum()\n",
    "        stat['action_loss'] = action_loss.item()\n",
    "\n",
    "        # value loss term\n",
    "        targets = returns\n",
    "        value_loss = (values - targets).pow(2).view(-1)\n",
    "        value_loss *= alive_masks\n",
    "        value_loss = value_loss.sum()\n",
    "        stat['value_loss'] = value_loss.item()\n",
    "\n",
    "        loss = action_loss + self.args.value_coeff * value_loss\n",
    "\n",
    "        if not self.args.continuous:\n",
    "            # entropy regularization term\n",
    "            entropy = 0\n",
    "            for i in range(len(log_p_a)):\n",
    "                entropy -= (log_p_a[i] * log_p_a[i].exp()).sum()\n",
    "            stat['entropy'] = entropy.item()\n",
    "            if self.args.entr > 0:\n",
    "                loss -= self.args.entr * entropy\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        return stat\n",
    "\n",
    "    def run_batch(self):\n",
    "        batch = []\n",
    "        self.stats = dict()\n",
    "        self.stats['num_episodes'] = 0\n",
    "        while len(batch) < self.args.batch_size:\n",
    "            if self.args.batch_size - len(batch) <= self.args.max_steps:\n",
    "                self.last_step = True\n",
    "            episode, episode_stat = self.get_episode()\n",
    "            # merge_stat(episode_stat, self.stats)\n",
    "            self.stats['num_episodes'] += 1\n",
    "            batch += episode\n",
    "\n",
    "        self.last_step = False\n",
    "        self.stats['num_steps'] = len(batch)\n",
    "        batch = Transition(*zip(*batch))\n",
    "        return batch, self.stats\n",
    "\n",
    "    def train_batch(self):\n",
    "        batch, stat = self.run_batch()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        s = self.compute_grad(batch)\n",
    "        merge_stat(s, stat)\n",
    "        for p in self.params:\n",
    "            if p._grad is not None:\n",
    "                p._grad.data /= stat['num_steps']\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GymWrapper(object):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.obs_space = self.env.observation_space\n",
    "        self.act_space = self.env.action_space\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.env\n",
    "    \n",
    "    def get_num_of_agents(self):\n",
    "        return self.env.n\n",
    "    \n",
    "    def get_shape_of_obs(self):\n",
    "        obs_shapes = []\n",
    "        for obs in self.obs_space:\n",
    "            if isinstance(obs, spaces.Box):\n",
    "                obs_shapes.append(obs.shape)\n",
    "        assert len(self.obs_space) == len(obs_shapes)\n",
    "        return obs_shapes\n",
    "        \n",
    "    def get_output_shape_of_act(self):\n",
    "        act_shapes = []\n",
    "        for act in self.act_space:\n",
    "            if isinstance(act, spaces.Discrete):\n",
    "                act_shapes.append(act.n)\n",
    "            elif isinstance(act, spaces.MultiDiscrete):\n",
    "                act_shapes.append(act.high - act.low + 1)\n",
    "            elif isinstance(act, spaces.Boxes):\n",
    "                assert act.shape == 1\n",
    "                act_shapes.append(act.shape)\n",
    "        return act_shapes\n",
    "    \n",
    "    def get_dtype_of_obs(self):\n",
    "        return [obs.dtype for obs in self.obs_space]\n",
    "    \n",
    "    def get_input_shape_of_act(self):\n",
    "        act_shapes = []\n",
    "        for act in self.act_space:\n",
    "            if isinstance(act, spaces.Discrete):\n",
    "                act_shapes.append(act.n)\n",
    "            elif isinstance(act, spaces.MultiDiscrete):\n",
    "                act_shapes.append(act.shape)\n",
    "            elif isinstance(act, spaces.Boxes):\n",
    "                assert act.shape == 1\n",
    "                act_shapes.append(act.shape)\n",
    "        return act_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-055e75127d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'This is the epoch: {} and the current advantage is: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8a0beefe67c3>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8a0beefe67c3>\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;31m# merge_stat(episode_stat, self.stats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_episodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8a0beefe67c3>\u001b[0m in \u001b[0;36mget_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# define the main process of exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2701\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    645\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario_name = 'simple_tag'\n",
    "# scenario_name = 'simple_world_comm'\n",
    "\n",
    "# load scenario from script\n",
    "scenario = scenario.load(scenario_name + \".py\").Scenario()\n",
    "# create world\n",
    "world = scenario.make_world()\n",
    "# create multiagent environment\n",
    "env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, info_callback=None, shared_viewer=True)\n",
    "\n",
    "env = GymWrapper(env)\n",
    "\n",
    "Args = namedtuple('Args', ['agent_num',\n",
    "                           'hid_size',\n",
    "                           'obs_size',\n",
    "                           'continuous',\n",
    "                           'action_dim',\n",
    "                           'comm_iters',\n",
    "                           'init_std',\n",
    "                           'lrate',\n",
    "                           'batch_size',\n",
    "                           'max_steps',\n",
    "                           'gamma',\n",
    "                           'mean_ratio',\n",
    "                           'normalize_rewards',\n",
    "                           'advantages_per_action',\n",
    "                           'value_coeff',\n",
    "                           'entr',\n",
    "                           'action_num'\n",
    "                          ]\n",
    "                 )\n",
    "\n",
    "args = Args(agent_num=env.get_num_of_agents(),\n",
    "            hid_size=100,\n",
    "            obs_size=np.max(env.get_shape_of_obs()),\n",
    "            continuous=False,\n",
    "            action_dim=np.max(env.get_output_shape_of_act()),\n",
    "            comm_iters=100,\n",
    "            init_std=0.2,\n",
    "            lrate=0.001,\n",
    "            batch_size=32,\n",
    "            max_steps=200,\n",
    "            gamma=0.99,\n",
    "            mean_ratio=0,\n",
    "            normalize_rewards=True,\n",
    "            advantages_per_action=False,\n",
    "            value_coeff=0.001,\n",
    "            entr=0.001,\n",
    "            action_num=np.max(env.get_input_shape_of_act())\n",
    "           )\n",
    "\n",
    "policy_net = CommNet(args)\n",
    "epoch = 0\n",
    "for i in range(1000):\n",
    "    train = Trainer(args, policy_net, env())\n",
    "    train.train_batch()\n",
    "    print ('This is the epoch: {} and the current advantage is: {}'.format(epoch, train.stats['action_loss']))\n",
    "    epoch += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('This is the scenario observation: \\n', dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (dir(env().observation_space[0]))\n",
    "print (dir(env().action_space[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (env()._get_obs(env().agents[1]))\n",
    "print (env()._get_reward(env().agents[1]))\n",
    "print (env().action_space)\n",
    "print (env().observation_space)\n",
    "print (env().agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
