Actor-Critic:

args = Args(agent_num=env.get_num_of_agents(),
            hid_size=64,
            obs_size=np.max(env.get_shape_of_obs()),
            continuous=False,
            action_dim=np.max(env.get_output_shape_of_act()),
            init_std=0.1,
            policy_lrate=1e-2,
            value_lrate=2e-2,
            epoch_size=32,
            max_steps=50,
            gamma=0.95,
            normalize_advantages=False,
            entr=1e-3,
            action_num=np.max(env.get_input_shape_of_act()),
            training_strategy=training_strategy,
            q_func=True,
            train_epoch_num=10000,
            replay_buffer_size=1e6,
            replay_iters=10,
            cuda=True,
            grad_clip=True,
            behaviour_update_freq=1,
            save_model_freq=10,
            replay=True,
            target_lr=1e-2,
            target_update_freq=2
           )

Reinforce:

args = Args(agent_num=env.get_num_of_agents(),
            hid_size=64,
            obs_size=np.max(env.get_shape_of_obs()),
            continuous=False,
            action_dim=np.max(env.get_output_shape_of_act()),
            init_std=0.1,
            policy_lrate=1e-2,
            value_lrate=2e-2,
            epoch_size=32,
            max_steps=50,
            gamma=0.95,
            normalize_advantages=False,
            entr=1e-3,
            action_num=np.max(env.get_input_shape_of_act()),
            training_strategy=training_strategy,
            q_func=True,
            train_epoch_num=10000,
            replay_buffer_size=1e6,
            replay_iters=10,
            cuda=True,
            grad_clip=True,
            behaviour_update_freq=1,
            save_model_freq=10,
            replay=False,
            target_lr=1e-2,
            target_update_freq=2
           )

DDPG:

args = Args(agent_num=env.get_num_of_agents(),
            hid_size=64,
            obs_size=np.max(env.get_shape_of_obs()),
            continuous=False,
            action_dim=np.max(env.get_output_shape_of_act()),
            init_std=0.1,
            policy_lrate=1e-2,
            value_lrate=2e-2,
            epoch_size=32,
            max_steps=50,
            gamma=0.95,
            normalize_advantages=False,
            entr=1e-3,
            action_num=np.max(env.get_input_shape_of_act()),
            training_strategy=training_strategy,
            q_func=True,
            train_epoch_num=10000,
            replay_buffer_size=1e6,
            replay_iters=1,
            cuda=True,
            grad_clip=True,
            behaviour_update_freq=1,
            save_model_freq=10,
            replay=True,
            target_lr=1e-2,
            target_update_freq=2
           )